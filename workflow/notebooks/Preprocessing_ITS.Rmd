---
title: "dada2 preprocessing of 2012 ITS data"
author: "Andreas Schneider"
date: "18/07/2019"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r import}
suppressMessages(library(here)) 
suppressMessages(library(dada2))
suppressMessages(library(ShortRead))
suppressMessages(library(Biostrings))
suppressMessages(library(dplyr))
suppressMessages(library(reshape2))
```

# Introduction

The data will be imported into R and clustered to ASV (amplicon sequencing variants) with the help of the dada2 R package.

Crucial difference of ITS compared to 16S: the ITS region is highly variable in length (due to fast mutation rate), and thus shouldn't be truncated at one length like the 16S region.

### Needles

```{r ITS_Needles }
path_n_i <- here("data/ITS/Needles/DeML_pooled/")

fnFs_n_i <- sort(list.files(path_n_i, pattern = "R1.fastq.gz", full.names = TRUE))
fnRs_n_i <- sort(list.files(path_n_i, pattern = "R2.fastq.gz", full.names = TRUE))
```

First we check for primers in the data (I think the primers were already removed in theses datasets, but we check just to make sure). First we record their DNA sequences.

```{r}
ITS1f <- "CTTGGTCATTTAGAGGAAGTAA"
ITS2 <- "GCTGCGTTCTTCATCGATGC"
```

Next we create a vector with all possible orientations of the primers

```{r}
allOrients <- function (primer) {
  require(Biostrings)
  dna <- DNAString(primer)
  orients <- c(Forward = dna, Complement = complement(dna), 
               Reverse = reverse(dna), RevComp = reverseComplement(dna))
  return(sapply(orients, toString))
}
ITS1f.orients <- allOrients(ITS1f)
ITS2.orients <- allOrients(ITS2)
```

The presence of ambiguous bases (Ns) in the sequencing reads makes accurate mapping of short primer sequences difficult. Next we are going to “pre-filter” the sequences just to remove those with Ns, but perform no other filtering.

```{r cache=TRUE}
fnFs_n_i.filtN <- file.path(path_n_i, "filtN", basename(fnFs_n_i))
fnRs_n_i.filtN <- file.path(path_n_i, "filtN", basename(fnRs_n_i))
filterAndTrim(fnFs_n_i, fnFs_n_i.filtN, fnRs_n_i, fnRs_n_i.filtN, maxN = 0, multithread = TRUE)
```

Now we can count the number of times the primer sequences appear in the samples.

```{r cache=TRUE}
primerHits <- function (primer, fn) {
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(ITS1f.orients, primerHits, fn = fnFs_n_i.filtN[[1]]),
      FWD.ReverseReads = sapply(ITS1f.orients, primerHits, fn = fnRs_n_i.filtN[[1]]),
      REV.ForwardReads = sapply(ITS2.orients, primerHits, fn = fnFs_n_i.filtN[[1]]),
      REV.ReverseReads = sapply(ITS2.orients, primerHits, fn = fnRs_n_i.filtN[[1]]))
```

There are a few hits after all, so we proceed with the cutting step.

```{r}
cutadapt <- "/mnt/picea/home/aschneider/.conda/envs/amp_seq/bin/cutadapt"
system2(cutadapt, args = "--version")
```

```{r cache=TRUE,include=FALSE}
path.cut <- file.path(path_n_i, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs_n_i.cut <- file.path(path.cut, basename(fnFs_n_i))
fnRs_n_i.cut <- file.path(path.cut, basename(fnRs_n_i))

ITS1f.rc <- rc(ITS1f)
ITS2.rc <- rc(ITS2)

#Trim FWD and revcomp of REV off of R1
R1.flags <- paste("-g", ITS1f, "-a", ITS2.rc)
#Trim rev and rc of fw off of R2
R2.flags <- paste("-G", ITS2, "-A", ITS1f.rc)
#Run Cutadapt
for (i in seq_along(fnFs_n_i)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, 
                             "-n", 2,
                             "--minimum-length", 50,
                             "-o", fnFs_n_i.cut[i],
                             "-p", fnRs_n_i.cut[i],
                             fnFs_n_i.filtN[i],
                             fnRs_n_i.filtN[i]))
}

rbind(FWD.ForwardReads = sapply(ITS1f.orients, primerHits, fn = fnFs_n_i.cut[[1]]),
      FWD.ReverseReads = sapply(ITS1f.orients, primerHits, fn = fnRs_n_i.cut[[1]]),
      REV.ForwardReads = sapply(ITS2.orients, primerHits, fn = fnFs_n_i.cut[[1]]),
      REV.ReverseReads = sapply(ITS2.orients, primerHits, fn = fnRs_n_i.cut[[1]]))
```
The sanity check looks good, all primer sequences have been removed from the data.

Now we can proceed. We start with extracting the sample names and plotting Quality profiles for the first 4 samples.

```{r cache=TRUE}
sample.names_n_i <- sapply(strsplit(basename(fnFs_n_i.cut), "_"), `[`, 1)

plotQualityProfile(fnFs_n_i.cut[1:4])
plotQualityProfile(fnRs_n_i.cut[1:4])
```

Next is the filtering and trimming.

```{r}
filtFs_n_i <- file.path(path_n_i, "filtered_F", basename(fnFs_n_i.cut))
filtRs_n_i <- file.path(path_n_i, "filtered_R", basename(fnRs_n_i.cut))
```
The dada2 recommends the following (standard) parameters for the filtering.
```{r cache=TRUE}
out_n_i <- filterAndTrim(fnFs_n_i.cut, filtFs_n_i, fnRs_n_i.cut, filtRs_n_i, maxN = 0, 
                         maxEE = c(6,6), truncQ = 2, minLen = 50, rm.phix = TRUE, 
                         compress = TRUE, multithread = TRUE)

head(out_n_i, n = 10)
```
Since the overall read quality is quite bad we had to go up to 6 with the mEE parameter to retain a decent number of reads.

The next steps (learning errors and the dada2 algorithm) are run from the command line again.

#### Chimera removal

```{r}
seq.tab_n_i <- readRDS("data/ITS/Needles/dada2/seqtab.rds")
seq.tab_n_i.nochim <- removeBimeraDenovo(seq.tab_n_i, method = "consensus",
                                       multithread= TRUE, verbose = TRUE)
saveRDS(seq.tab_n_i.nochim, "data/ITS/Needles/dada2/seqtab_nochim.rds")

#Import dada_files
dadaFs_n_i <- readRDS("data/ITS/Needles/dada2/dada_f.rds")
dadaRs_n_i <- readRDS("data/ITS/Needles/dada2/dada_r.rds")
#mergers_n_i <- mergePairs(dadaFs_n_i, filtFs_n_i, dadaRs_n_i, filtRs_n_i, verbose = TRUE, maxMismatch = 1)

```

```{r}
#track the reads through pipeline
track_n_i <- cbind(out_n_i, sapply(dadaFs_n_i, getN), sapply(dadaRs_n_i, getN), rowSums(seq.tab_n_i), rowSums(seq.tab_n_i.nochim))
colnames(track_n_i) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track_n_i) <- sample.names_n_i
head(track_n_i, n = 15)
```

#### Writing of sequence fasta for ITSx

```{r}
dna_n_i <- DNAStringSet(colnames(seq.tab_n_i.nochim))
names(dna_n_i) <- paste0("ASV", seq(ncol(seq.tab_n_i.nochim)))
writeXStringSet(dna_n_i, file = "data/ITS/Needles/phyloseq/refseq.fa")
```

#### Import ITSx sequences and collate (if possible/necessary)

```{r}
dna_n_i_clean <- as.character(readDNAStringSet(here("data/ITS/Needles/ITSx/2012needles.ITS1.full_and_partial.fasta")))
seq.tab_n_i.nochim2 <- seq.tab_n_i.nochim
colnames(seq.tab_n_i.nochim2) <- names(dna_n_i)
seq.tab_n_i.nochim2 <- seq.tab_n_i.nochim2[,colnames(seq.tab_n_i.nochim2)%in%names(dna_n_i_clean)]

colnames(seq.tab_n_i.nochim2) <- dna_n_i_clean
seq.tab_n_i.nochim2 <- t(seq.tab_n_i.nochim2)
table(nchar(getSequences(t(seq.tab_n_i.nochim2))))

#There is one sequence below 50bp, it needs to be purged, either now or later. Now is better
dna_n_i_clean <- dna_n_i_clean[!(nchar(dna_n_i_clean)<50)]
seq.tab_n_i.nochim2 <- seq.tab_n_i.nochim2[nchar(rownames(seq.tab_n_i.nochim2))>50,]

#At this point we can summarise all identical sequences 
seq.tab_n_i.nc3 <- cbind.data.frame(sequence=rownames(seq.tab_n_i.nochim2), seq.tab_n_i.nochim2)
seqtab_n_i.nc4 <- group_by(seq.tab_n_i.nc3, sequence) %>% 
  summarise_each(funs(sum)) 
seqtab_n_i.nc5 <- seqtab_n_i.nc4[,-c(1,2)]
rownames(seqtab_n_i.nc5) <- seqtab_n_i.nc4$sequence
seqtab_n_i.nc5 <- data.matrix(t(seqtab_n_i.nc5))
saveRDS(seqtab_n_i.nc5, here("data/ITS/Needles/ITSx/seqtab_clean.rds"))
```



### Roots

```{r ITS_Roots }
path_r_i <- here("data/ITS/Roots/DeML_pooled/")

fnFs_r_i <- sort(list.files(path_r_i, pattern = "R1.fastq.gz", full.names = TRUE))
fnRs_r_i <- sort(list.files(path_r_i, pattern = "R2.fastq.gz", full.names = TRUE))
```

First we check for primers in the data (I think the primers were already removed in theses datasets, but we check just to make sure).

The presence of ambiguous bases (Ns) in the sequencing reads makes accurate mapping of short primer sequences difficult. Next we are going to “pre-filter” the sequences just to remove those with Ns, but perform no other filtering.

```{r cache=TRUE}
fnFs_r_i.filtN <- file.path(path_r_i, "filtN", basename(fnFs_r_i))
fnRs_r_i.filtN <- file.path(path_r_i, "filtN", basename(fnRs_r_i))
filterAndTrim(fnFs_r_i, fnFs_r_i.filtN, fnRs_r_i, fnRs_r_i.filtN, maxN = 0, multithread = TRUE)
```

Now we can count the number of times the primer sequences appear in the samples.

```{r cache=TRUE}

rbind(FWD.ForwardReads = sapply(ITS1f.orients, primerHits, fn = fnFs_r_i.filtN[[1]]),
      FWD.ReverseReads = sapply(ITS1f.orients, primerHits, fn = fnRs_r_i.filtN[[1]]),
      REV.ForwardReads = sapply(ITS2.orients, primerHits, fn = fnFs_r_i.filtN[[1]]),
      REV.ReverseReads = sapply(ITS2.orients, primerHits, fn = fnRs_r_i.filtN[[1]]))
```

There are a few hits after all, so we proceed with the cutting step.

```{r cache=TRUE,include=FALSE}
path.cut_r <- file.path(path_r_i, "cutadapt")
if(!dir.exists(path.cut_r)) dir.create(path.cut_r)
fnFs_r_i.cut <- file.path(path.cut_r, basename(fnFs_r_i))
fnRs_r_i.cut <- file.path(path.cut_r, basename(fnRs_r_i))

#Run Cutadapt
for (i in seq_along(fnFs_r_i)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, 
                             "-n", 2,
                             "--minimum-length", 50,
                             "-o", fnFs_r_i.cut[i],
                             "-p", fnRs_r_i.cut[i],
                             fnFs_r_i.filtN[i],
                             fnRs_r_i.filtN[i]))
}

rbind(FWD.ForwardReads = sapply(ITS1f.orients, primerHits, fn = fnFs_r_i.cut[[1]]),
      FWD.ReverseReads = sapply(ITS1f.orients, primerHits, fn = fnRs_r_i.cut[[1]]),
      REV.ForwardReads = sapply(ITS2.orients, primerHits, fn = fnFs_r_i.cut[[1]]),
      REV.ReverseReads = sapply(ITS2.orients, primerHits, fn = fnRs_r_i.cut[[1]]))
```
The sanity check looks good, all primer sequences have been removed from the data.

Now we can proceed as with the 16S data. We start with extracting the sample names and plotting Quality profiles for the first 4 samples.

```{r cache=TRUE}
sample.names_r_i <- sapply(strsplit(basename(fnFs_r_i.cut), "_"), `[`, 1)

plotQualityProfile(fnFs_r_i.cut[1:4])
plotQualityProfile(fnRs_r_i.cut[1:4])


```

Looks better than the needles. Next is the filtering and trimming.

```{r}
filtFs_r_i <- file.path(path_r_i, "filtered_F", basename(fnFs_r_i.cut))
filtRs_r_i <- file.path(path_r_i, "filtered_R", basename(fnRs_r_i.cut))
```
The dada2 recommends the following (standard) parameters for the filtering. `maxEE` was changed to 6, otherwise we lose too much data.
```{r cache=TRUE}
out_r_i <- filterAndTrim(fnFs_r_i.cut, filtFs_r_i, fnRs_r_i.cut, filtRs_r_i, maxN = 0, 
                         maxEE = c(6,6), truncQ = 2, minLen = 50, rm.phix = TRUE, 
                         compress = TRUE, multithread = TRUE)

head(out_r_i, n = 10)

```

Denoising and merging ran on SLURM.

#### Chimera removal

```{r}
seq.tab_r_i <- readRDS("data/ITS/Roots/dada2/seqtab.rds")
seq.tab_r_i.nochim <- removeBimeraDenovo(seq.tab_r_i, method = "consensus",
                                       multithread= TRUE, verbose = TRUE)
saveRDS(seq.tab_r_i.nochim, "data/ITS/Roots/dada2/seqtab_nochim.rds")

#Import dada_files
dadaFs_r_i <- readRDS("data/ITS/Roots/dada2/dada_f.rds")
dadaRs_r_i <- readRDS("data/ITS/Roots/dada2/dada_r.rds")
#mergers_r_i <- mergePairs(dadaFs_r_i, filtFs_r_i, dadaRs_r_i, filtRs_r_i, verbose = TRUE, maxMismatch = 1)

```

```{r}
#track the reads through pipeline
track_r_i <- cbind(out_r_i, sapply(dadaFs_r_i, getN), sapply(dadaRs_r_i, getN), rowSums(seq.tab_r_i), rowSums(seq.tab_r_i.nochim))
colnames(track_r_i) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track_r_i) <- sample.names_r_i
head(track_r_i, n = 15)
```

#### Writing of sequence fasta for ITSx

```{r}
dna_r_i <- DNAStringSet(colnames(seq.tab_r_i.nochim))
names(dna_r_i) <- paste0("ASV", seq(ncol(seq.tab_r_i.nochim)))
writeXStringSet(dna_r_i, file = "data/ITS/Roots/phyloseq/refseq.fa")
```

#### Import ITSx sequences and collate (if possible/necessary)

```{r}
dna_r_i_clean <- as.character(readDNAStringSet(here("data/ITS/Roots/ITSx/2012roots.ITS1.full_and_partial.fasta")))
#This isn't right
seq.tab_r_i.nochim2 <- seq.tab_r_i.nochim
colnames(seq.tab_r_i.nochim2) <- names(dna_r_i)
seq.tab_r_i.nochim2 <- seq.tab_r_i.nochim2[,colnames(seq.tab_r_i.nochim2)%in%names(dna_r_i_clean)]

colnames(seq.tab_r_i.nochim2) <- dna_r_i_clean
seq.tab_r_i.nochim2 <- t(seq.tab_r_i.nochim2)
table(nchar(getSequences(t(seq.tab_r_i.nochim2))))

#There are a few sequences below 50bp, they needs to be purged, either now or later. Now is better
dna_r_i_clean <- dna_r_i_clean[!(nchar(dna_r_i_clean)<50)]
seq.tab_r_i.nochim2 <- seq.tab_r_i.nochim2[nchar(rownames(seq.tab_r_i.nochim2))>50,]

#At this point we can summarise all identical sequences 
seq.tab_r_i.nc3 <- cbind.data.frame(sequence=rownames(seq.tab_r_i.nochim2), seq.tab_r_i.nochim2)
seqtab_r_i.nc4 <- group_by(seq.tab_r_i.nc3, sequence) %>% 
  summarise_each(funs(sum)) 
seqtab_r_i.nc5 <- seqtab_r_i.nc4[,-c(1,2)]
rownames(seqtab_r_i.nc5) <- seqtab_r_i.nc4$sequence
seqtab_r_i.nc5 <- data.matrix(t(seqtab_r_i.nc5))
saveRDS(seqtab_r_i.nc5, here("data/ITS/Roots/ITSx/seqtab_clean.rds"))
```


### Merging Needles and Roots

Now that both datasets have been filtered with ITSx, we can merge them before we cluster the data with swarm.

```{r merge_n_r}
#seqtab_BOTH <- mergeSequenceTables(seqtab_n_i.nc5, seqtab_r_i.nc5)
seqtab_BOTH <- mergeSequenceTables(readRDS(here("data/ITS/Needles/ITSx/seqtab_clean.rds")), readRDS(here("data/ITS/Roots/ITSx/seqtab_clean.rds")))
```

#### Export cut and summed ASVs for Swarm

Swarm needs abundance info included, I will use the sum of counts in all samples. Since I have evaluated the Mock elsewhere, it has been removed at this point to prevent that it interfers with the clustering.

```{r}
seqtab_BOTH <- seqtab_BOTH[,colSums(seqtab_BOTH)>0]
dna_i_clean_sum2 <- DNAStringSet(colnames(seqtab_BOTH))
names(dna_i_clean_sum2) <- paste0("ASV", 1:length(dna_i_clean_sum2))
names(dna_i_clean_sum2) <- paste0(names(dna_i_clean_sum2), ";size=", colSums(seqtab_BOTH))
writeXStringSet(dna_i_clean_sum2, file = here("data/ITS/Both/swarm/ASVs_cut_sum_abun.fa"))
```

Now we can run swarm on these cut ASVs with the summed abundances, and re-import the clusters it creates.

I will import the Swarm clusters as a factor that will enable a summarisation.

```{r}
clustfact <- function(file) {
  clus <- readr::read_tsv(file, col_names = FALSE)
  clus <- stringr::str_split_fixed(clus$X1, pattern = " ", n = max(stringr::str_count(clus$X1, "ASV")))
  rownames(clus) <- paste0("cluster", 1:nrow(clus))
  clus <- melt(as.matrix(clus))[,-2]
  clus <- clus[grepl("ASV", clus$value),]
  clus$value <- gsub(";size=\\d+", "", clus$value)
  #clus2 <- as.factor(clus$Var1)
  #names(clus2) <- clus$value
  colnames(clus) <- c("Cluster", "ASV")
  #clus$Cluster <- as.character(clus$Cluster)
  return(clus)
}
clus_swarm <- clustfact(here("data/ITS/Both/swarm/results.txt"))

```

Now we can add this factor as a column to our matrix, and summarise the counts by cluster.

```{r}
seqtab_BOTH_sw <- as.data.frame(cbind(Cluster=clus_swarm$Cluster[match(gsub(";size=\\d+", "", names(dna_i_clean_sum2)), clus_swarm$ASV)], t(seqtab_BOTH)))

seqtab_BOTH_sw <- group_by(seqtab_BOTH_sw, Cluster) %>% 
  summarise_each(funs(sum)) 
seqtab_BOTH_sw$Cluster <- paste0("Cluster", seqtab_BOTH_sw$Cluster)
seqtab_BOTH_sw2 <- seqtab_BOTH_sw
rownames(seqtab_BOTH_sw2) <- seqtab_BOTH_sw$Cluster
seqtab_BOTH_sw2 <- data.matrix(seqtab_BOTH_sw2[,-1])
```

The counts are now summarised by cluster. Next we can assign taxonomy to these SOTUs (Swarm Operational Taxonomic Units) and then start with the actual analyses.

```{r}
dna_i_clean_swarm <- readDNAStringSet(here("data/ITS/Both/swarm/seeds.fasta"))
names(dna_i_clean_swarm) <- seqtab_BOTH_sw$Cluster 
saveRDS(dna_i_clean_swarm, here("data/ITS/Both/swarm/seqs.rds"))
```


